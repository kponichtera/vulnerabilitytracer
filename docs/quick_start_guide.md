# Quick start guide

The following guide describes the fundamentals of working with the project,
from starting the experimental setup to building and running developed solution in it.

All relevant operations can be executed with [Task](https://taskfile.dev/). 
To see all documented tasks, execute `task -l`.
To show all the tasks (including the ones, executed by other tasks), execute `task -a`.

## Prerequisites 

### Hardware requirements

* 4-core CPU (8 cores recommended)
* 48 GB of RAM (64 GB recommended)

### For running experimental setup

* [Task](https://taskfile.dev/)
* [Skaffold](https://skaffold.dev/)
* [kubectl](https://kubernetes.io/docs/reference/kubectl/)
* [kustomize](https://kustomize.io/)
* [Helm](https://helm.sh/)
* [Istioctl](https://istio.io/latest/docs/reference/commands/istioctl/)
* [k3d](https://k3d.io/)
* [Telepresence](https://www.telepresence.io/)
* [Docker](https://www.docker.com/)
  * Can use different container runtime, compatible with k3d
* [FASTEN](https://github.com/fasten-project/fasten) system running locally and accessible on the loopback interface (127.0.0.1)

### For developing components of the solution

* Instrumentation agent and experimental microservices
  * Java Development Kit 11
  * Gradle (can use the attached wrapper)
* Vulnerability Tracer backend components
  * Go 1.21
  * Tools from the [install_go_tools.sh](../vulnerabilitytracer-service/docker/builder/install_go_tools.sh) script
* Vulnerability Tracer web dashboard
  * NodeJS
  * npm
* Load generator
  * Rust (2021 edition)
* Experiment analysis
  * Python 3.11.8
  * Dependencies from the [requirements.txt](../misc/plots/requirements.txt) file

## Launching Kubernetes cluster

Make sure that Docker (or alternative k3d-compatible runtime) is running.
Then, start the cluster with:

```shell
task cluster:start
```

The local k3d Kubernetes cluster will be started, along with the caches for the common container image registries
(like Docker Hub, ghcr.io, quay.io), as well as the local registry where the built images will be deployed to by Skaffold.

If you modify configuration of one of the cluster components, you can reconfigure them by executing:

```shell
task cluster:configure
```

You can also reconfigure the components in a granular way, 
by executing appropriate task (for example, execute `task cluster:configure:istio` to reconfigure Istio).

### Accessing cluster's control plane

Once the cluster starts, its kubeconfig file will be generated in the _.kube/config_ directory in the project's root.
To authenticate with the local cluster and execute commands using standard Kubernetes tooling (like kubectl or Helm),
export the KUBECONFIG environment variable:

```shell
export KUBECONFIG=.kube/config
```

## Launching developed solution

Vulnerability Tracer is deployed to the local cluster with Skaffold.
To build its OpenTelemetry agent extension and make it available within the cluster, execute:

```shell
task opentelemetry:run
```

To build and deploy remaining components of Vulnerability Tracer, execute:

```shell
task vulnerabilitytracer:run
```

Alternatively, both of them can be deployed with:

```shell
task run
```

One the Vulnerability Tracer is up and running, 
you can access its web dashboard on http://dashboard.vulnerabilitytracer.localhost:8080.

## Launching experiments

To execute the most representative experiment that generates data for Vulnerability Tracer, execute:

```shell
task experiment:run
```

The remaining scenarios for qualitative and quantitative evaluation of Vulnerability Tracer are defined 
in the [skaffold-experiment.yaml](../skaffold-experiment.yaml) Skaffold file as separate profiles.
You can list them with `task -l` and look at the tasks with names, 
starting from `experiment:qualitative-scenario` and `experiment:quantitative-scenario`.

> **IMPORTANT:** Do not run more than one experiment scenario at the same time.
> Before executing a different scenario, make sure that all the previous ones were deleted
> by executing `task experiment:clean`

## Enabling communication with local FASTEN instance

To allow the coordinator module of Vulnerability Tracer to communicate with the local instance of FASTEN,
you have to intercept the communication with a dummy `fasten-api` Service within the Kubernetes cluster
by using Telepresence:

```shell
task telepresence:fasten-api:intercept
```

## Accessing cluster components

The components of experimental setup are exposed on the local loopback interface.
The following ones have a web interface and can be accessed from the browser:

| Tool                           | URL                                                 | Purpose                                                                     |
|--------------------------------|-----------------------------------------------------|-----------------------------------------------------------------------------|
| Kiali                          | http://kiali.istio.localhost:8080                   | Exploring service mesh status                                               |
| Grafana                        | http://grafana.lgtm.localhost:8080                  | Exploring logs, metrics and traces                                          |
| MinIO                          | http://console.minio.localhost:8080                 | Exploring content of the object storage buckets, used by cluster components |
| Vulnerability Tracer dashboard | http://dashboard.vulnerabilitytracer.localhost:8080 | Working with Vulnerability Tracer                                           |

The following components have no web interface, but their API can be accessed on the loopback interface.
This can be useful for executing requests, generated from the OpenAPI specification, or by developing components locally.

| Tool                             | URL                                                   | Purpose                                                                     |
|----------------------------------|-------------------------------------------------------|-----------------------------------------------------------------------------|
| Vulnerability Tracer coordinator | http://coordinator.vulnerabilitytracer.localhost:8080 | Exploring service mesh status                                               |
| Vulnerability Tracer manager     | http://manager.vulnerabilitytracer.localhost:8080     | Exploring logs, metrics and traces                                          |
| Grafana Tempo query frontend     | http://tempo.lgtm.localhost:8080                      | Exploring content of the object storage buckets, used by cluster components |

Additionally, you can access the PostgreSQL database of Vulnerability Tracer 
by using the following configuration with any PostgreSQL client:

* host: 127.0.0.1 (localhost)
* user: vulnerabilitytracer
* database: vulnerabilitytracer
* password: obtained by executing `task cluster:database:get-password`